{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73c1500a"
      },
      "source": [
        "# 2026 Visual AI Hackathon Enablement Kit: Manufacturing & Workplace Safety\n",
        "\n",
        "## 1. Objective\n",
        "This project aims to build a demo application and tutorial that serves as the primary **\"Enablement Asset\"** for the **CVPR 2026 Worker Safety Challenge**. It serves as a semantic dataset curator and visualizer.\n",
        "\n",
        "The asset demonstrates an end-to-end example workflow between **TwelveLabs** and **FiftyOne**, providing a tool for participants to build a high-quality, small-data training set from raw footage without manual framing.\n",
        "\n",
        "> **Strategic Goal**: Demonstrate that \"Small Data\" does not mean \"Manual Data.\" We aim to show how modern semantic search can replace hours of manual video scrubbing.\n",
        "\n",
        "## 2. Challenge Context\n",
        "*   **Event**: Visual AI Hackathon 2026.\n",
        "*   **Track**: Challenge Track (Worker Safety).\n",
        "\n",
        "### The \"Enablement\" Gap\n",
        "Participants will see an end-to-end workflow utilizing:\n",
        "1.  **Marengo 3.0 Vector Embedding Generation**: For multimodal understanding.\n",
        "2.  **Pegasus Cluster Metadata & Identification**: For zero-shot auto-labeling.\n",
        "3.  **Voxel51 UI Data Curation and Visualizer**: For interactive exploration.\n",
        "\n",
        "By using this general semantic data curator, participants will gain hands-on exposure to the underlying API and SDK for both platforms, avoiding the high latency of 40+ hours of manual video scrubbing.\n",
        "\n",
        "## 3. Setup and Dependencies\n",
        "The following cell installs the necessary Python packages: `fiftyone`, `twelvelabs`, `python-dotenv`, and `torch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fiftyone\n",
        "!pip install twelvelabs\n",
        "!pip install umap-learn\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77f5c7a5"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Let's go ahead and set up our TwelveLabs API key. You'll need to create an account on the TwelveLabs platform, which you can do here: https://playground.twelvelabs.io/\n",
        "\n",
        "You can then [create an API key here](https://playground.twelvelabs.io/dashboard/api-keys), making sure that you save the API key somewhere safe as you won't be able to view it again.\n",
        "\n",
        "Once you've done that, run the following cell and enter your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "TL_API_KEY = getpass(\"Enter TwelveLabs API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2560a086"
      },
      "source": [
        "## Download Dataset and Parse into FiftyOne \n",
        "\n",
        "We'll start by downloading an example dataset, note this dataset will consume ~9.3GB in disk space. \n",
        "\n",
        "The quickest way to download the dataset is by pulling it from the [Voxel51 Hugging Face org](https://huggingface.co/Voxel51). You can do that as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "dataset = load_from_hub(\n",
        "    \"Voxel51/Safe_and_Unsafe_Behaviours\",\n",
        "    name=\"safe_unsafe_behaviours\",\n",
        "    persistent=True,\n",
        "    overwrite=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is a FiftyOne Dataset? \n",
        "\n",
        "It's composed of multiple Sample objects which contain Field attributes, all of which can be dynamically created, modified and deleted. FiftyOne uses a lightweight non-relational database to store datasets, so you can easily scale to datasets of any size without worrying about RAM constraints on your machine.\n",
        "\n",
        "\n",
        "In the event that you encounter some rate limiting from Hugging Face, it's advised that you download and parse the dataset manually. You can download the dataset [here](https://data.mendeley.com/datasets/xjmtb22pff/1), or by running the following command in your terminal: `wget -O dataset.zip \"https://data.mendeley.com/public-api/zip/xjmtb22pff/download/1\"`\n",
        "\n",
        "Once the data is downloaded, [importing it to FiftyOne](https://docs.voxel51.com/user_guide/import_datasets.html) is quite straight forward. \n",
        "\n",
        "You'll need to unzip and extract the dataset and then follow along with the instructions below.\n",
        "\n",
        "This dataset is in video classification directory tree format, so we can [use the appropriate loader](https://docs.voxel51.com/user_guide/import_datasets.html#video-classification-dir-tree) as follows:\n",
        "\n",
        "```python\n",
        "import fiftyone as fo\n",
        "\n",
        "base_dir = \"Video Dataset for Safe and Unsafe Behaviours/Safe and Unsafe Behaviours Dataset\"\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=f\"{base_dir}/test\",\n",
        "    dataset_type=fo.types.VideoClassificationDirectoryTree,\n",
        "    tags=[\"test\"]\n",
        ")\n",
        "\n",
        "# Create train dataset\n",
        "train_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=f\"{base_dir}/train\",\n",
        "    dataset_type=fo.types.VideoClassificationDirectoryTree,\n",
        "    tags=[\"train\"]\n",
        ")\n",
        "```\n",
        "\n",
        "We can then combine the datasets using the [`add_collections` method](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.add_collection) of the [FiftyOne Dataset](https://docs.voxel51.com/user_guide/using_datasets.html):\n",
        "\n",
        "```python\n",
        "train_dataset.add_collection(test_dataset)\n",
        "train_dataset.name = \"safe_unsafe_behaviours\"\n",
        "train_dataset.persistent = True\n",
        "```\n",
        "\n",
        "Then, we can map the labels (which come from the subdirectory each [Sample](https://docs.voxel51.com/api/fiftyone.core.sample.html#fiftyone.core.sample.Sample) lives in) to a more human readable format using the [`map_labels` method](https://docs.voxel51.com/api/fiftyone.core.view.html#fiftyone.core.view.DatasetView.map_labels) of the Dataset.\n",
        "\n",
        "```python\n",
        "label_mapping = {\n",
        "    '0_safe_walkway_violation': 'Safe Walkway Violation',\n",
        "    '1_unauthorized_intervention': 'Unauthorized Intervention',\n",
        "    '2_opened_panel cover': 'Opened Panel Cover',\n",
        "    '3_carrying_overload_with_forklift': 'Carrying Overload with Forklift',\n",
        "    '4_safe_walkway': 'Safe Walkway',\n",
        "    '5_authorized_intervention': 'Authorized Intervention',\n",
        "    '6_closed_panel_cover': 'Closed Panel Cover',\n",
        "    '7_safe_carrying': 'Safe Carrying'\n",
        "}\n",
        "\n",
        "\n",
        "view = train_dataset.map_labels(\"ground_truth\", label_mapping)\n",
        "view.save()\n",
        "```\n",
        "\n",
        "Finally, you will want to use the [`compute_metadata` method](https://docs.voxel51.com/user_guide/using_datasets.html#metadata) of the Dataset. When you run `compute_metadata()` on a video dataset, FiftyOne populates each sample’s `metadata` field with a `VideoMetadata` object containing at least the following fields:\n",
        "\n",
        "- `size_bytes` – file size in bytes  \n",
        "- `mime_type` – MIME type (e.g. `video/mp4`)  \n",
        "- `frame_width` – width of the video frames in pixels  \n",
        "- `frame_height` – height of the video frames in pixels  \n",
        "- `frame_rate` – frames per second  \n",
        "- `total_frame_count` – total number of frames  \n",
        "- `duration` – duration in seconds  \n",
        "- `encoding_str` – codec/encoding string (e.g. `avc1`)\n",
        "\n",
        "[View the docs here](https://docs.voxel51.com/user_guide/basics.html) to learn more about the basics of FiftyOne Datasets, and the docs here for the [specifics of video datasets](https://docs.voxel51.com/user_guide/using_datasets.html#video-datasets).\n",
        "\n",
        "Once the Dataset has been parsed to FiftyOne, if you need to access it again in another session you can simply call:\n",
        "\n",
        "```python\n",
        "import fiftyone as fo\n",
        "\n",
        "dataset = fo.load_dataset(\"safe_unsafe_behaviours\")\n",
        "```\n",
        "\n",
        "If you need to [delete the Dataset](\n",
        "https://docs.voxel51.com/user_guide/using_datasets.html#deleting-a-dataset), say because you reran the code here and FiftyOne tells you the Dataset name already exists, all you have to do is open the terminal and run: `fiftyone datasets delete safe_unsafe_behaviours`. Note this won't delete the files from your local disk, just the reference to it in the FiftyOne database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3490abb"
      },
      "source": [
        "## Initialize TwelveLabs Index\n",
        "\n",
        "Now we'll connect to TwelveLabs and create an **index** to store our video data. Think of an index as a container that holds all your indexed videos along with their generated embeddings and metadata.\n",
        "\n",
        "### What is an Index?\n",
        "\n",
        "A [TwelveLabs Index](https://docs.twelvelabs.io/docs/concepts/indexes) is a searchable collection of videos. When you upload a video to an index, TwelveLabs processes it through the configured models to generate:\n",
        "\n",
        "- **Vector embeddings**: Dense numerical representations that capture the semantic meaning of video content\n",
        "- **Temporal segments**: The video is divided into meaningful chunks for fine-grained search\n",
        "- **Multimodal understanding**: Both visual and audio tracks are analyzed\n",
        "\n",
        "This enables powerful capabilities like semantic search (\"find moments where workers are not wearing helmets\") and video-to-text generation.\n",
        "\n",
        "### Models We're Using\n",
        "\n",
        "We'll configure our index with two complementary models:\n",
        "\n",
        "| Model | Type | Purpose |\n",
        "|-------|------|---------|\n",
        "| [**Marengo 3.0**](https://docs.twelvelabs.io/v1.3/docs/concepts/models/marengo) | Embedding | Generates rich multimodal embeddings from video. Processes both `visual` and `audio` modalities to create 1024-dimensional vectors that capture semantic content. These embeddings power similarity search and clustering. |\n",
        "| [**Pegasus 1.2**](https://docs.twelvelabs.io/v1.3/docs/concepts/models/pegasus) | Generative | Video-to-text model that can analyze video content and generate natural language descriptions. We'll use this later for zero-shot labeling of our clusters. |\n",
        "\n",
        "### Under the Hood\n",
        "\n",
        "When we call [`indexes.create()`](https://docs.twelvelabs.io/v1.3/docs/concepts/indexes#create-an-index), TwelveLabs provisions cloud infrastructure to:\n",
        "1. Accept video uploads via the Tasks API\n",
        "2. Run the configured models on each video\n",
        "3. Store the resulting embeddings in a vector database\n",
        "4. Enable fast similarity search across all indexed videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0jd8TTN7aJl",
        "outputId": "e2a3a39e-6133-4310-e7ea-fe871f997317"
      },
      "outputs": [],
      "source": [
        "from twelvelabs import TwelveLabs\n",
        "from twelvelabs.indexes import IndexesCreateRequestModelsItem\n",
        "\n",
        "TL_INDEX_NAME = \"fiftyone-twelvelabs-index\"\n",
        "\n",
        "# Create or retrieve TwelveLabs index\n",
        "twelvelabs_client = TwelveLabs(api_key=TL_API_KEY)\n",
        "\n",
        "def get_twelvelabs_index(index_name: str) -> int:\n",
        "    \"\"\"\n",
        "    Returns the ID of the TwelveLabs index with the given name.\n",
        "    If the index does not exist, it creates a new index with the given name.\n",
        "    \"\"\"\n",
        "    indexes = twelvelabs_client.indexes.list()\n",
        "    for index in indexes:\n",
        "        if index.index_name == TL_INDEX_NAME:\n",
        "            print(\"Found index with name {} with ID {}\".format(TL_INDEX_NAME, index.id))\n",
        "            return index.id\n",
        "    index = twelvelabs_client.indexes.create(\n",
        "        index_name=TL_INDEX_NAME,\n",
        "        models=[\n",
        "            IndexesCreateRequestModelsItem(\n",
        "                model_name=\"marengo3.0\", model_options=[\"visual\", \"audio\"]\n",
        "            ),\n",
        "            IndexesCreateRequestModelsItem(\n",
        "                model_name=\"pegasus1.2\", model_options=[\"visual\", \"audio\"]\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    print(\"Created index with name {} with ID {}\".format(TL_INDEX_NAME, index.id))\n",
        "    return index.id\n",
        "\n",
        "index_id = get_twelvelabs_index(TL_INDEX_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1668ed1"
      },
      "source": [
        "## Video Ingestion and Indexing\n",
        "This step uploads videos to TwelveLabs using FiftyOne views to filter the dataset:\n",
        "\n",
        "1.  **Filtering**: Selects videos from the target split with duration ≥ 4 seconds that haven't been indexed yet.\n",
        "\n",
        "2.  **Sampling**: Takes up to `VIDEOS_PER_LABEL` videos per label.\n",
        "\n",
        "3.  **Indexing**: Uploads each video to TwelveLabs and stores the `tl_video_id` on the sample.\n",
        "\n",
        "The cell is idempotent—rerunning it will skip already-indexed videos.\n",
        "\n",
        "First, let's define some variables and a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XzjpqPy7bgk",
        "outputId": "b34f0f16-3de0-4d04-e633-467065ae9b57"
      },
      "outputs": [],
      "source": [
        "# Only run this cell if videos have not been indexed already.\n",
        "import json\n",
        "from fiftyone import ViewField as F\n",
        "\n",
        "def index_video_to_twelvelabs(sample):\n",
        "    \"\"\"\n",
        "    Upload a FiftyOne video sample to TwelveLabs for indexing.\n",
        "    \n",
        "    This function reads the video file from disk, uploads it to TwelveLabs,\n",
        "    and waits for the indexing task to complete. The video is indexed using\n",
        "    the Marengo 3.0 model which generates visual and audio embeddings.\n",
        "    \n",
        "    Args:\n",
        "        sample: A FiftyOne Sample object with 'filepath' and 'ground_truth.label' fields.\n",
        "        \n",
        "    Returns:\n",
        "        str: The TwelveLabs video_id assigned to the indexed video.\n",
        "        \n",
        "    Raises:\n",
        "        Exception: If the indexing task fails (status != \"ready\").\n",
        "    \"\"\"\n",
        "    # Read video bytes from the sample's filepath\n",
        "    with open(sample.filepath, \"rb\") as f:\n",
        "        # Create an indexing task in TwelveLabs\n",
        "        # user_metadata stores FiftyOne sample info for later cross-referencing\n",
        "        task = twelvelabs_client.tasks.create(\n",
        "            index_id=index_id,\n",
        "            video_file=f.read(),\n",
        "            user_metadata=json.dumps({\n",
        "                \"filepath\": sample.filepath,\n",
        "                \"sample_id\": sample.id,\n",
        "                \"label\": sample.ground_truth.label\n",
        "            })\n",
        "        )\n",
        "    \n",
        "    # Block until TwelveLabs finishes processing the video\n",
        "    task = twelvelabs_client.tasks.wait_for_done(task_id=task.id)\n",
        "    \n",
        "    # Verify the task completed successfully\n",
        "    if task.status != \"ready\":\n",
        "        raise Exception(f\"Task failed: {task.status}\")\n",
        "    \n",
        "    # Retrieve and return the assigned video_id\n",
        "    return twelvelabs_client.tasks.retrieve(task_id=task.id).video_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll use FiftyOne's powerful [view operations](https://docs.voxel51.com/user_guide/using_views.html) to filter and iterate through our dataset. \n",
        "\n",
        "### The Filtering Pipeline\n",
        "\n",
        "We build a view with three filters chained together:\n",
        "\n",
        "1. **`match_tags(DATASET_SPLIT)`** - Select only samples tagged with our target split (e.g., \"train\")\n",
        "\n",
        "2. **`match(F(\"metadata.duration\") >= MIN_DURATION)`** - Keep videos at least 4 seconds long (TwelveLabs requirement)\n",
        "\n",
        "3. **`match(~F(\"tl_video_id\").exists())`** - Exclude videos we've already indexed (idempotency)\n",
        "\n",
        "The `F()` syntax is FiftyOne's [ViewField](https://docs.voxel51.com/api/fiftyone.core.expressions.html) expression language, which lets you filter on any field in your samples.\n",
        "\n",
        "### Stratified Sampling\n",
        "\n",
        "We then loop through each unique label and use `.take(VIDEOS_PER_LABEL)` to limit how many videos we index per class. This ensures balanced representation across categories while keeping API costs manageable during development.\n",
        "\n",
        "You can learn more about Views in [this cheat sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html) and Filtering [in this cheat sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MIN_DURATION = 4.0\n",
        "VIDEOS_PER_LABEL = 3\n",
        "DATASET_SPLIT = \"train\"\n",
        "\n",
        "# Filter: correct split, sufficient duration, not yet indexed\n",
        "base_view = (\n",
        "    dataset\n",
        "    .match_tags(DATASET_SPLIT)\n",
        "    .match(F(\"metadata.duration\") >= MIN_DURATION)\n",
        "    .match(~F(\"tl_video_id\").exists())\n",
        ")\n",
        "\n",
        "# Index up to VIDEOS_PER_LABEL samples per label\n",
        "for label in base_view.distinct(\"ground_truth.label\"):\n",
        "    label_view = base_view.match(F(\"ground_truth.label\") == label).take(int(VIDEOS_PER_LABEL))\n",
        "    print(f\"\\n{label}: {len(label_view)} to index\")\n",
        "    \n",
        "    for sample in label_view.iter_samples(autosave=True, progress=True):\n",
        "        try:\n",
        "            sample[\"tl_video_id\"] = index_video_to_twelvelabs(sample)\n",
        "            print(f\"  ✓ {sample.filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ {sample.filename}: {e}\")\n",
        "\n",
        "print(f\"\\nTotal indexed: {len(dataset.exists('tl_video_id'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e785d41"
      },
      "source": [
        "## Fetch Embeddings and Populate Dataset\n",
        "\n",
        "Now that our videos are indexed in TwelveLabs, we can [retrieve the **visual embeddings**](https://docs.twelvelabs.io/v1.3/api-reference/create-embeddings-v1/video-embeddings/retrieve-video-embeddings) generated by Marengo 3.0. These are 512-dimensional vectors that encode the semantic content of each video.\n",
        "\n",
        "### What are Video Embeddings?\n",
        "\n",
        "When TwelveLabs indexes a video, Marengo 3.0 processes the visual and audio streams to produce dense vector representations. These embeddings capture high-level semantic information like:\n",
        "\n",
        "- Actions and activities occurring in the video\n",
        "- Objects and people present\n",
        "- Scene context and environment\n",
        "- Temporal patterns and motion\n",
        "\n",
        "Videos with similar content will have embeddings that are close together in vector space, enabling similarity search and clustering.\n",
        "\n",
        "### Efficient Retrieval Pattern\n",
        "\n",
        "We use FiftyOne's optimized iteration pattern:\n",
        "\n",
        "- [**`select_fields(\"tl_video_id\")`**](tl_embedding) - Only loads the field we need, avoiding unnecessary memory usage\n",
        "- [**`iter_samples(autosave=True)`**](https://docs.voxel51.com/api/fiftyone.core.view.html#fiftyone.core.view.DatasetView) - Batches database writes efficiently instead of saving after each sample\n",
        "\n",
        "The embeddings are stored on each sample as `tl_embedding` for persistence, and also collected in a list for the clustering step that follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X666A3PH7dZF",
        "outputId": "e2cb72c9-4fd0-4db9-9116-f859389ca67b"
      },
      "outputs": [],
      "source": [
        "def fetch_embedding(video_id: str) -> list:\n",
        "    \"\"\"Fetch visual embedding for a TwelveLabs video.\"\"\"\n",
        "    video_info = twelvelabs_client.indexes.videos.retrieve(\n",
        "        index_id=index_id,\n",
        "        video_id=video_id,\n",
        "        embedding_option=[\"visual\"]\n",
        "    )\n",
        "    return video_info.embedding.video_embedding.segments[0].float_\n",
        "\n",
        "# Work with indexed samples (those with tl_video_id)\n",
        "indexed_view = dataset.exists(\"tl_video_id\")\n",
        "print(f\"Fetching embeddings for {len(indexed_view)} indexed videos...\")\n",
        "\n",
        "# Efficient iteration: select_fields avoids loading unnecessary data,\n",
        "# autosave=True lets FiftyOne batch the writes efficiently\n",
        "embeddings = []\n",
        "for sample in indexed_view.select_fields(\"tl_video_id\").iter_samples(\n",
        "    progress=True, autosave=True\n",
        "):\n",
        "    embedding = fetch_embedding(sample.tl_video_id)\n",
        "    sample[\"tl_embedding\"] = embedding\n",
        "    embeddings.append(embedding)  # Keep in memory for clustering later\n",
        "\n",
        "print(f\"\\nStored {len(embeddings)} embeddings on dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can confirm the length of the embedding as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(indexed_view.first()['tl_embedding'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04100e24"
      },
      "source": [
        "## Semantic Clustering and Auto-Labeling\n",
        "\n",
        "To achieve \"Small Data\" curation without manual effort:\n",
        "\n",
        "1.  **KMeans Clustering**: We cluster the video embeddings into 8 distinct groups based on semantic similarity.\n",
        "\n",
        "2.  **Pegasus Generation**: For each cluster, we use the **TwelveLabs Pegasus 1.2** model to generate a [descriptive label](https://docs.twelvelabs.io/v1.3/docs/get-started/quickstart/analyze-videos)\n",
        "\n",
        "3.  **Annotation**: These labels are applied to all samples in the cluster.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mAAJnTBQaWn",
        "outputId": "a9eb6dd2-82a0-47b3-dd1a-1ee20b2c1931"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "CLUSTER_LABEL_PROMPT = \"\"\"\n",
        "Analyze this workplace safety video and classify it as exactly ONE of the following labels.\n",
        "\n",
        "UNSAFE BEHAVIORS (violations):\n",
        "- Safe Walkway Violation: Worker walking outside the designated safe walkway boundaries, entering restricted or hazardous areas\n",
        "- Unauthorized Intervention: Worker intervening on equipment/machinery WITHOUT wearing required safety gear (intervention vest)\n",
        "- Opened Panel Cover: Electrical/machinery panel cover left open after intervention\n",
        "- Carrying Overload with Forklift: Forklift carrying 3 or more blocks\n",
        "\n",
        "SAFE BEHAVIORS (compliance):\n",
        "- Safe Walkway: Worker staying within the designated safe walkway boundaries\n",
        "- Authorized Intervention: Worker intervening on equipment/machinery while wearing proper safety gear (intervention vest)\n",
        "- Closed Panel Cover: Electrical/machinery panel cover properly closed after intervention\n",
        "- Safe Carrying: Forklift carrying 2 or fewer blocks\n",
        "\n",
        "Return ONLY the exact label name, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "def generate_label(video_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a semantic label for a video using TwelveLabs Pegasus 1.2.\n",
        "    \n",
        "    This function uses TwelveLabs' video-to-text analysis to generate a \n",
        "    descriptive label for a video. The label is intended to represent the \n",
        "    video's content in the context of workplace safety (violations or \n",
        "    good practices).\n",
        "    \n",
        "    We use this to auto-label clusters: one representative video from each \n",
        "    cluster is analyzed, and the generated label is applied to all videos \n",
        "    in that cluster.\n",
        "    \n",
        "    Args:\n",
        "        video_id: The TwelveLabs video ID to analyze.\n",
        "        \n",
        "    Returns:\n",
        "        A single label string with underscores instead of spaces \n",
        "        (e.g., \"Unsafe_Walking_Path\", \"Proper_PPE_Usage\").\n",
        "        \n",
        "    Note:\n",
        "        Uses temperature=0.2 for more deterministic/consistent outputs.\n",
        "        See: https://docs.twelvelabs.io/v1.3/docs/get-started/quickstart/analyze-videos\n",
        "    \"\"\"\n",
        "    result = twelvelabs_client.analyze(\n",
        "        video_id=video_id,\n",
        "        prompt=CLUSTER_LABEL_PROMPT,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return result.data\n",
        "\n",
        "# Cluster embeddings using KMeans\n",
        "num_clusters = 8\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "\n",
        "cluster_labels = kmeans.fit_predict(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Cluster Labels\n",
        "\n",
        "Now we generate a human-readable label for each cluster using Pegasus 1.2. Rather than labeling every video individually (expensive and slow), we:\n",
        "\n",
        "1. Pick one representative video from each cluster\n",
        "2. Ask Pegasus to analyze it and generate a descriptive label\n",
        "3. Apply that label to all videos in the cluster\n",
        "\n",
        "This is the \"zero-shot auto-labeling\" that makes this workflow scalable—8 API calls instead of 24.\n",
        "\n",
        "### Extracting Field Values Efficiently\n",
        "\n",
        "Since we only need the `tl_video_id` field for this step, we use FiftyOne's [`values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.values) method to extract just that field as a Python list:\n",
        "\n",
        "`video_ids = indexed_view.values(\"tl_video_id\")`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cluster_label_map = {}\n",
        "cluster_strings = []\n",
        "\n",
        "video_ids = indexed_view.values(\"tl_video_id\")\n",
        "\n",
        "for video_id, cluster_idx in zip(video_ids, cluster_labels):\n",
        "    if cluster_idx not in cluster_label_map:\n",
        "        print(f\"Generating label for cluster {cluster_idx}...\")\n",
        "        cluster_label_map[cluster_idx] = generate_label(video_id)\n",
        "        print(f\"  -> {cluster_label_map[cluster_idx]}\")\n",
        "    cluster_strings.append(cluster_label_map[cluster_idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batch Update with `set_values()`\n",
        "\n",
        "Finally, we use [`set_values()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values) to assign the cluster labels to all samples in a single operation.\n",
        "\n",
        "### Storing as Classification Labels\n",
        "\n",
        "Rather than storing plain strings, we convert our cluster labels to [`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "classifications = [fo.Classification(label=s) for s in cluster_strings]\n",
        "\n",
        "indexed_view.set_values(\"pred_cluster\", classifications)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect the results as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indexed_view.first().pred_cluster.label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indexed_view.first().ground_truth.label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having the predicitions stored as clusters allows you to use FiftyOne's [built-in methods for evaluation](https://docs.voxel51.com/user_guide/evaluation.html#evaluating-models).\n",
        "\n",
        "For example, we can [evaluate the results of the classification](https://docs.voxel51.com/user_guide/evaluation.html#classifications) as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the predictions in the `predictions` field with respect to the\n",
        "# labels in the `ground_truth` field\n",
        "results = indexed_view.evaluate_classifications(\n",
        "    \"pred_cluster\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval_simple\",\n",
        ")\n",
        "\n",
        "# Print a classification report\n",
        "results.print_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd0c0c7"
      },
      "source": [
        "## Visualization\n",
        "We use **FiftyOne Brain** to compute a 2D visualization (UMAP) of the embeddings.\n",
        "Finally, we launch the **FiftyOne App**, allowing you to explore the clusters, view the auto-generated labels, and analyze the dataset interactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3fba805b69344377990a8519ddf430e7",
            "f612712c003b41418c932a9438a519d8",
            "5a0c767b27394017b566f4d30ef0176c",
            "e2b6be05e00e4156b5822e4537ba8085",
            "536d931ea8fa4296bab9c16a22aa3a61",
            "928b960c304a4043a85e73af924fd6fa",
            "79760e6c678f431aa71a4e73167024b8",
            "a3a51594ee094cd5a532358fa3fc6411",
            "2edf659df1844b9e845197b8ecf38cbe",
            "0b811dc85f964b7c8036148f241f7df0",
            "0df9e90ac89a4b5ea648a3eafb32a336"
          ]
        },
        "id": "JNjKkRtDbxqc",
        "outputId": "2dd0c4a0-70b4-4cb4-b728-1a79f5370938"
      },
      "outputs": [],
      "source": [
        "# Create 2D UMAP visualization of embeddings\n",
        "import fiftyone as fo\n",
        "import fiftyone.brain as fob\n",
        "\n",
        "results = fob.compute_visualization(\n",
        "    indexed_view,\n",
        "    embeddings=\"tl_embedding\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"tl_embeddings_viz\",\n",
        "    method=\"umap\",\n",
        "    verbose=True,\n",
        "    seed=51,\n",
        ")\n",
        "\n",
        "# Launch the FiftyOne App to explore the dataset\n",
        "session = fo.launch_app(dataset, auto=False, port=5151)\n",
        "session.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73d1d643"
      },
      "source": [
        "## 6. Export to PyTorch Dataset\n",
        "\n",
        "Now we convert our FiftyOne dataset into a PyTorch `Dataset` for training a classifier on the embeddings.\n",
        "\n",
        "### Why `to_torch()`?\n",
        "\n",
        "FiftyOne provides a [`to_torch()`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_torch) method that wraps any view in a PyTorch Dataset interface. This is cleaner than manually extracting lists because:\n",
        "\n",
        "- **Lazy loading** - Samples are transformed on-access, not all loaded into memory upfront\n",
        "\n",
        "- **Reusable** - The same `GetItem` works with any view (train/val splits, filtered subsets)\n",
        "\n",
        "- **Decoupled** - Transformation logic lives in one place, separate from data extraction\n",
        "\n",
        "### The `GetItem` Pattern\n",
        "\n",
        "We define a [`GetItem`](https://docs.voxel51.com/api/fiftyone.utils.torch.html#fiftyone.utils.torch.GetItem) subclass that tells FiftyOne:\n",
        "\n",
        "1. **What fields to extract** (`required_keys`)\n",
        "\n",
        "2. **How to transform them** (`__call__`)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "114db26a",
        "outputId": "7d82dde0-2372-4ebf-9438-d0a08cc300ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created PyTorch dataset with 24 samples\n",
            "Embedding shape: torch.Size([512])\n",
            "Label: Safe Walkway Violation (idx: 0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from fiftyone.utils.torch import GetItem\n",
        "\n",
        "\n",
        "# Define label mapping (string label → integer index)\n",
        "LABEL_TO_IDX = {\n",
        "    \"Safe Walkway Violation\": 0,\n",
        "    \"Unauthorized Intervention\": 1,\n",
        "    \"Opened Panel Cover\": 2,\n",
        "    \"Carrying Overload with Forklift\": 3,\n",
        "    \"Safe Walkway\": 4,\n",
        "    \"Authorized Intervention\": 5,\n",
        "    \"Closed Panel Cover\": 6,\n",
        "    \"Safe Carrying\": 7,\n",
        "}\n",
        "\n",
        "\n",
        "class WorkerSafetyGetItem(GetItem):\n",
        "    \"\"\"\n",
        "    Extracts embeddings and labels from FiftyOne samples for classifier training.\n",
        "    \n",
        "    Transforms each sample into:\n",
        "    - embedding: 512-dim tensor from TwelveLabs Marengo\n",
        "    - label_idx: integer class index\n",
        "    - label_str: human-readable label string\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, label_to_idx, field_mapping=None):\n",
        "        self.label_to_idx = label_to_idx\n",
        "        super().__init__(field_mapping=field_mapping)\n",
        "    \n",
        "    @property\n",
        "    def required_keys(self):\n",
        "        # Fields we need from each sample\n",
        "        return [\"tl_embedding\", \"ground_truth\"]\n",
        "    \n",
        "    def __call__(self, d):\n",
        "        embedding = d.get(\"tl_embedding\")\n",
        "        ground_truth = d.get(\"ground_truth\")\n",
        "        label_str = ground_truth.label\n",
        "        \n",
        "        return {\n",
        "            \"embedding\": torch.tensor(embedding, dtype=torch.float32),\n",
        "            \"label_idx\": torch.tensor(self.label_to_idx.get(label_str, -1), dtype=torch.long),\n",
        "            \"label_str\": label_str,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Then `indexed_view.to_torch(getter)` gives us a PyTorch Dataset where `dataset[i]` calls our `GetItem` to load and transform sample `i`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the PyTorch dataset from FiftyOne\n",
        "getter = WorkerSafetyGetItem(LABEL_TO_IDX)\n",
        "torch_dataset = indexed_view.to_torch(getter)\n",
        "\n",
        "print(f\"Created PyTorch dataset with {len(torch_dataset)} samples\")\n",
        "\n",
        "# Verify a sample\n",
        "sample = torch_dataset[0]\n",
        "print(f\"Embedding shape: {sample['embedding'].shape}\")\n",
        "print(f\"Label: {sample['label_str']} (idx: {sample['label_idx']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Custom Collation for DataLoader\n",
        "\n",
        "When PyTorch's `DataLoader` batches samples together, it needs to know how to combine them. The default collate works for simple tensors, but our samples contain a mix of:\n",
        "- **Tensors** (`embedding`, `label_idx`) - need to be stacked\n",
        "- **Strings** (`label_str`) - need to stay as a list\n",
        "\n",
        "A custom [`collate_fn`](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) tells the DataLoader exactly how to handle each field. This gives us batches ready for training—embeddings as a `[batch_size, 512]` tensor and labels as a `[batch_size]` tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch embeddings: torch.Size([4, 512])\n",
            "Batch labels: tensor([3, 7, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader for training\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"embedding\": torch.stack([b[\"embedding\"] for b in batch]),\n",
        "        \"label_idx\": torch.stack([b[\"label_idx\"] for b in batch]),\n",
        "        \"label_str\": [b[\"label_str\"] for b in batch],\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    torch_dataset, \n",
        "    batch_size=4, \n",
        "    shuffle=True, \n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Verify a batch\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch embeddings: {batch['embedding'].shape}\")\n",
        "print(f\"Batch labels: {batch['label_idx']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You now have a standard PyTorch `DataLoader` that yields batches ready for your training loop.\n",
        "\n",
        "## Summary\n",
        "\n",
        "This notebook demonstrated an end-to-end workflow from raw video to trainable dataset:\n",
        "\n",
        "| Step | Tool | What Happens |\n",
        "|------|------|--------------|\n",
        "| **1. Ingest** | FiftyOne | Load videos with metadata and ground truth labels |\n",
        "| **2. Index** | TwelveLabs | Upload videos, generate Marengo 3.0 embeddings |\n",
        "| **3. Retrieve** | TwelveLabs | Fetch 512-dim embeddings back to FiftyOne samples |\n",
        "| **4. Cluster** | scikit-learn | Group similar videos using KMeans on embeddings |\n",
        "| **5. Auto-label** | TwelveLabs Pegasus | Generate semantic labels for each cluster |\n",
        "| **6. Export** | FiftyOne `to_torch()` | Convert to PyTorch Dataset for training |\n",
        "\n",
        "The key insight: **\"Small Data\" doesn't mean \"Manual Data.\"** By combining TwelveLabs' semantic understanding with FiftyOne's data management, you can build high-quality training sets from raw footage without hours of manual video scrubbing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fiftyone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b811dc85f964b7c8036148f241f7df0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df9e90ac89a4b5ea648a3eafb32a336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2edf659df1844b9e845197b8ecf38cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fba805b69344377990a8519ddf430e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f612712c003b41418c932a9438a519d8",
              "IPY_MODEL_5a0c767b27394017b566f4d30ef0176c",
              "IPY_MODEL_e2b6be05e00e4156b5822e4537ba8085"
            ],
            "layout": "IPY_MODEL_536d931ea8fa4296bab9c16a22aa3a61"
          }
        },
        "536d931ea8fa4296bab9c16a22aa3a61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a0c767b27394017b566f4d30ef0176c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3a51594ee094cd5a532358fa3fc6411",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2edf659df1844b9e845197b8ecf38cbe",
            "value": 500
          }
        },
        "79760e6c678f431aa71a4e73167024b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928b960c304a4043a85e73af924fd6fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a51594ee094cd5a532358fa3fc6411": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b6be05e00e4156b5822e4537ba8085": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b811dc85f964b7c8036148f241f7df0",
            "placeholder": "​",
            "style": "IPY_MODEL_0df9e90ac89a4b5ea648a3eafb32a336",
            "value": " 500/500 [00:00]"
          }
        },
        "f612712c003b41418c932a9438a519d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928b960c304a4043a85e73af924fd6fa",
            "placeholder": "​",
            "style": "IPY_MODEL_79760e6c678f431aa71a4e73167024b8",
            "value": "Epochs completed: 100%| "
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
